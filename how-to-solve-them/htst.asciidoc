= How To Solve Them
Notes about solving algorithmic problems
:stem:

== Data Structures

=== Heaps

Insertion, deletion stem:[O(log n)]
Lookup: stem:[O(1)]

C++ use a priority_queue. Header is queue.
min heap has the smallest element on the top and larger elements below, therefore it uses a greater
function for its comparator.

--------
    priority_queue<int, vector<int>, function<bool(int,int)>>
        min_heap([](int a, int b)->bool{return a > b;});
--------

max heap has the largest element on the top and smaller elements below therefore use a less function
for its comparator

=== Stacks and Queues

* A stack behaves like a queue with its inputs reversed.
* Finding the min/max of a dynamic stream, use a min heap and a max heap.


=== Arrays
O(1) lookup.

=== Graphs

* BFS is good for shortest path.
  - Need a queue to track the nodes to visit.
  - Tracking the distance can be tricky. Can use a marker. So after adding children of a node, add the
    mark value. When you pop the mark value inc the depth count. Or use 2 queues a current and next
    queue. While popping the current queue, add those children to the next queue. In the next
    iteration swap queues.

* Graphs can be represented as adjacency lists or an adjaceny matrix.
  - adj list is economical in space. A simple adj list is an array of lists. The index in the array
    is the node, the list at that index contains the nodes that this node joins to.
  - DFS on a graph is O(V + E) i.e. the sum of the number of edges and vertices.
  - BFS is also O(V + E)


=== Linked lists

O(n) lookup.

=== Trees, graphs, tries

* trie or prefix tree is good for word completion type problems.

=== Hash tables

O(1) lookup.

unorded_map is C++ hash table map is implement via a rb-tree.
Basic hash table implemented via an array and a hash function.
Basic hash function is mod.
Good for keeping counts of unique items
CHaining where a collision is handled by appending to the end of the bucket
Open addressing where the new value is written to the next available slot.

=== Binary Search Tree

== Algorithms

=== Dijkstra

=== Bellman-Ford

=== A*

=== Kadane's Algorithm

This simple looking piece of code is really brilliant. The reasoning behind it is amazing and
a demonstration of inductive reasoning and the ability to ask the right question. The
problem it solves is 'find the maximum sum subarray in an array'. What this means is that we want
the maximum sum (not the subarray) given by this subarray. So for example given an array

---------
[-1,-2,5]
---------

Then the answer is 5.

Your function signature will look something like this:

-----------------
int max_sum_subarray(int* arr)
-----------------

A naive approach calculates all the sums for all subarrays and tracks the maximum. This is
stem:[O(n^3)]. Now from the initial framing of the question you might think that knowing the
subarray is important. Surely you need to know each subarray to calculate it? However this isn't the
case. Kadane's genius was to realize that what the subarray is, isn't important. All that matters is
that there is a maximum. Additionally every solution will always have a last element. So assume that
we know the maximum at stem:[A\[i\]]. Call it stem:[B_i]. Then the solution at the next i,
stem:[B_(i+1)] is stem:[max(B_i + A\[i+1\], B_i)]. This leads to the following algorithm:

------------
int max_so_far = 0
int max_now = 0;

max_now = max_so_far  = arr[0];
for (int i = 1; i < arr.size(); ++i)
{
  max_now = max(max_so_far + arr[i], arr[i]);
  max_so_far = max(max_so_far, max_now);
}

------------

=== Binary Search

== Patterns behind the problem

=== Dynamic programming

* Problems often involve choice. If the problem requires you to make a choice then the solution
  probably involves dynamic programming.
* Solutions are based off their recursive expressions. You create a table or an array to record the
  values calculated during a previous run of the function. This benefits recursive functions that
  call themselves more than once. For example a fibboncci function f(n) = f(n-1) + f(n-2) calls
  itself more  than once.
* Come up with the recursive formulation, then memoize it afterwards.

=== Divide and Conquer

* Cut the problem in half and solve each half. Example is quicksort or mergesort.

=== Recursion

* Recursive functions can be written non-recursively using a stack
* Recursive functions are easier to reason about verbally (i.e. at a higher level). Thinking about
  mechanically becomes hard. For example an in-order bst print is `print left tree, print this node,
  print right tree`

=== Search and sort
* Breadth first search uses a queue to maintain the nodes that we want to visit next
  - It's good for find the shortest path to a node.
* Arrays are simple hash tables.
* Searching algos are generally `log(n)`
* Sorting algos are generally `nlog(n)`

=== Numbers
* stem:[2^10 = 1024 ~~] one kilo, stem:[2^20 = 1048576 ~~] one mega,
  stem:[2^30 = 1073741824 ~~] one giga
* Remember that the midpoint between 2 points in an array, L and H, is stem:[L + ((H-L)/2)]. Think of it
  as the average of 2 points.
* Max value of an unsigned 32 bit integer is stem:[2^32 - 1]. Number of elements in those 32 bits is
  stem:[2^32]
* To select the rightmost bit of x: `x&~(x - 1)`. For example let x = 110 then x-1 = 101, then
  `~(x-1) = 10` so 110 & 010 = 010. Let x = 111011 then x - 1 = 111010, `~(x-1) = 000101` and
  `x&~(x-1) = 1`. That idea can be used to clear the rightmost bit as well by not using the
  complement: `x&(x-1)`

== General notes
* Problems you've seen before might come disguised as something else. Don't be fooled by the back story.
* Think about the brute force solution 1st. It's often better than to come up with that if you get stuck
* Ask clarifying questions. Don't assume too much.
* A good interview means that you've explained things well. A great one means that the interviewer
  learned something from you.
* Some questions will combine data structures. For example a cache is a combination of a list and a
  hash table.
* Sliding window problems are tricky. Hard to think about how it works efficiently. For example how
  to use a sliding window to maintain the maximum. Naive way is to use a heap that is the size of
  the window. But the O(n) way is to use a list that keeps the maximum at the front.

== System Design Notes

* Remember to clarify the question. Question will be deliberately vague. Clarify inputs, use cases,
  number of users.
* Break the problem down into sections until they can be tackled by an appropriate algorithm or map
  to something tangible, like a database schema.
* Distributed hashing places the key at the node f(key) = key%n. This doesn't scale
  horizontally since if I add another node to the system the all keys need to be remapped.
  Consistent hashing resolves this problem (dynamo uses this technique) to minimize the number of
  keys that need to be re-distributed. Reduces to k/n.
* Caching lets you retrieve recent requests at the request node without putting pressure on the
  systems downstream. An effective cache will relieve the load on the downstream services. In a
  multiple node system each node can have a part of the cache locally (distributed caching) or we
  can have a global cache that all nodes request from. The global cache can be a dedicated machine
  with high-end hardware so it can serve the multiple requests it gets from the request layer of
  nodes. Cache invalidation occurs on writes. Can use a few policies 1) write through where the
  cache and origin are updated at the same time. Increased latency since 2 writes occur before
  returning. Advantage is we know that the system is consitent. 2) write around where we write to the
  backend and ignore the cache. This reduces load on the cache but if a subsequent read for that
  data comes through it means a cache miss and having to read it from the origin. Cache eviction
  occurs on read. A read request for an item not in the cache will require a read from the origin
  data store and an update made to the cache, thus evicting some existing key-value. LRU least
  recently used evicts based on items that were least recently used. LFU least frequently used keeps
  a count that
* CAP (consitency, availability, partition tolerence). Consistency means at all times is the view of
  the data the same. Availability means all requests return some data. Partition tolerance means if
  my cluster is split I can still serve read/write requests. Can have 2 out of 3.
  - Imagine a 2 node system. Link between 2 nodes breaks. You have 2 options 1) forgo consistency
    and allow requests to both nodes. 2) Forgo availability and make one node responsible for all
    read/write requests.

== Problems encountered

=== Write a memory allocator

Write a function to track memory allocated. Assume that you have a very large chunk of memory. Write
a function that allocates a portion of this chunk. This function can be used more than once, so
somehow it needs to record which parts of the larger block are in use, so that the next time it's
called the pointer to the block does not overlap with another section that is in use.

----
void* allocate(uint32_t size)
----

=== Decode ways

A message containing letters from A-Z is being encoded to numbers using the following mapping: 'A' -> 1,'B' -> 2 ...'Z' -> 26
Given a non-empty string containing only digits, determine the total number of ways to decode it.

* Came up with a recursive solution. Basically the number of ways (N_n) to get to then end of the
  string (S_n) is the number of ways to get to the previous S_n-1 + the number of ways to get to
  S_n-2 (only if those letters n-2 + 1 and n-2+2 are in the alphabet).
* Had trouble getting the memoization right. I cached the string as it decreased between recursive
  function calls BUT you don't need to memoize the string. You can just memoize the length and map
  that to the number of ways to get to that length. For example, given "12" then memo[0] = 1,
  memo[1] = 2.


